{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959a1c22-f630-4ca4-bec1-05d18c93488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no.of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#------------------------------ STEP 1 - CREATING TOKENS -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1702e8a-2a85-4de5-a756-06d4d513a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no.of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open (\"the-verdict.txt\" , \"r\" , encoding  = \"utf-8\" ) as f:\n",
    "    raw_text = f.read()\n",
    "print(\"total no.of character:\" , len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db33cfa8-83db-4af9-9b15-e2ca290e7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re - regular expression .\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1fc1a31-71c5-4f0c-9294-9228431a08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world. this, is a test.\"\n",
    "result = re.split(r'(\\s)' , text)   #splited text by white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b8c47c-3ac9-4e44-8ae8-88fc7e9abe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'this,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf9296e-6c93-4d9a-9d88-096a5cfbb5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'this', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "#spliting by white spaces as well as comas and fullstop\n",
    "result=re.split(r'([,.]|\\s)' , text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed310042-d312-4951-afe7-80404007282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'this', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "#excluding white spaces as these are also being counted as token\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dd65b2e-f0dd-43ad-bef4-b1e44b277c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello, world. is this-- a test?\"\n",
    "result=re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9236a587-ea35-4c1e-8104-3bc88eedb802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#for white spaces\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa75cd0a-c07e-440f-a067-d67df3807278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d996723b-f279-4c1d-bf5c-c5e2dc9173b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206dd7ef-fb5b-48b0-ae77-7ff88d4ea3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- STEP2 - CREATEING TOKEN ID'S --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf343bc-e0a5-468b-b042-5218d8b8879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this step firstly we have to arranged these in voacb order means in sorted way and each unique token is mapped to an unique integer calles token id (if 'the' is appered 2 times will be counted only once )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "362dce55-9e28-417a-9203-84402eff3d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))            #sorted in alphabatical order\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)         #only consist of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bcaedf8-0739-4317-ae32-ba4d71a36382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c572ce1-642e-4c87-b608-bc9d05200312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i , item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f38b2cc-f410-42b3-8ddd-fab8320d3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZER CLASS:\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)\n",
    "\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text = re.sub(r's+([,.?!\"()\\'])' , r'\\1' , text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "525c6b30-6d26-4d6b-a8c3-3380a62ed4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8ebf1dc-f11c-4b8a-960d-5b0fe85277bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODE : sample text -> tokenized text -> token id\n",
    "#DECODE : token id -> tokenized text -> sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df91a5b2-ddc2-42b9-8baa-cfc67b427c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It \\' s the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25d966b4-79a6-4892-ab4b-6f5944f0664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWO - SPECIAL TOKENS : <|unk|> - unknown , <|endoftext|> \n",
    "# if we provide some text which is not present in the dataset provided rather than having error in it we go with 'unkown' token \n",
    "# when we have multiple text sources we need to add endoftext token so that llm should understand it more effecitvely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d5f3747-e84f-4f9e-ab55-340baf5427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "special_tokens = {\n",
    "    \"<|endoftext|>\": len(all_tokens),\n",
    "    \"<|unk|>\": len(all_tokens) + 1\n",
    "}\n",
    "\n",
    "#all_tokens.extend([\"<|endoftext|>\" , \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "vocab.update(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f9f58b8-59b4-462c-bcf2-22ac2bd318f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4dad0e59-4800-4d47-8324-a7629f1eec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "#printing last 5 entries of updated vocab:\n",
    "\n",
    "for i , item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "321a2e7d-0063-4602-b9bb-830df4fdd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)\n",
    "\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text = re.sub(r's+([,.?!\"()\\'])' , r'\\1' , text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9bb0176-1a86-4cc3-a42f-665cdf457a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, do you like tea?<|endoftext|>in the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"hello, do you like tea?\"\n",
    "text2 = \"in the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1,text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08968458-5060-4dd7-8215-c3f5659fb31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9d56d56-4973-450f-b279-05f80fc6ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> , do you like tea ? <|unk|> the sunlit terraces of the <|unk|> .'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5b2dc-083d-4048-a439-7c12c24bdfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9a847-02b7-44fe-ab0f-0b76321d5b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37020acd-6b8e-410e-ba00-254b42412a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce5cae-1019-4888-be59-a6ba2bd7c1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25991122-105f-4ce4-acd8-4428f92a1fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440169f1-6bff-4704-9079-9d8639aff355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce07d34-0815-4d34-a3c5-122634d8e038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13178134-a0b5-4f77-998d-29f6025185f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256460f0-5eb9-4770-b8b2-003c47523e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
